# -*- coding: utf-8 -*-
"""churn_random_forest_model.pkl

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KXPx0OwYM5k0GdTpBWgUQsftrPVXxtjM
"""

# importing all the necessary libraries

import pandas as pd, matplotlib.pyplot as plt, seaborn as sns

from google.colab import files
uploaded=files.upload()

# load the dataset
df=pd.read_excel('Churn.xlsx')
df

# dropping the first column
df=df.drop(columns=['Unnamed: 0'])

df.columns

# printing first 5 rows
df.head()

# printing last 5 rows
df.tail()

# checking for the shape of the dataset
df.shape

# get the basic information about the dataset
df.info()

# Converting 'day.charge' and 'eve.mins' into numerical
df['day.charge'] = pd.to_numeric(df['day.charge'], errors='coerce')
df['eve.mins'] = pd.to_numeric(df['eve.mins'], errors='coerce')

# descriptive statistics
df.describe()

# check for unique values
df.nunique()

# Check the distribution of categorical columns
categorical_cols = ['state', 'area.code', 'voice.plan', 'intl.plan', 'churn']
value_counts = {col: df[col].value_counts() for col in categorical_cols}
value_counts

# check for null values
df.isnull().sum()

# Drop rows with Null values in the specific columns
df = df.dropna(subset=['day.charge', 'eve.mins'])

df.isnull().sum()

# to check outliers
plt.figure(figsize=(12,9))
sns.boxplot(df,palette='inferno')
plt.xticks(rotation=90)
plt.show()

numerical_cols=df.select_dtypes(include=['int64','float64']).columns
numerical_cols

# Detect and print outliers using IQR method
for col in numerical_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]

    if not outliers.empty:
        print(f"Outliers detected in {col}:\n{outliers}\n")

# Remove outliers for each numeric column
for col in numerical_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]

# Check the shape of the data after removing outliers
print(f"Data shape after removing outliers: {df.shape}")

plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='churn',color='Gold')
plt.title('Distribution of Churn')
plt.show()

plt.figure(figsize=(15,9))
sns.pairplot(df,palette='viridis',hue='churn')
plt.show()

plt.figure(figsize=(12,9))
sns.heatmap(df.corr(numeric_only=True),annot=True,cmap='cividis')
plt.xticks(rotation=90)
plt.show()

df.hist(figsize=(12,9),color='Hotpink')
plt.tight_layout()
plt.show()

plt.figure(figsize=(12,9))
sns.boxplot(df,palette='inferno')
plt.xticks(rotation=90)
plt.show()

# Bar plot of churn by state
plt.figure(figsize=(15, 8))
sns.set(style="whitegrid")
sns.countplot(data=df, x='state', hue='churn', palette='magma')
plt.title('Churn Distribution by State')
plt.xlabel('State')
plt.ylabel('Customer Count')
plt.xticks(rotation=90)
plt.show()

"""FEATURE ENGINEERING

ENCODING 1)ONE HOT ENCODING
"""

# One-Hot Encoding for Categorical Variables
df = pd.get_dummies(df, columns=['voice.plan', 'intl.plan','churn'], drop_first=True)
df.head()

"""2) LABEL ENCODING"""

from sklearn.preprocessing import LabelEncoder

# Create a label encoder object
le = LabelEncoder()

# Apply label encoding to 'state' and 'area.code'
df['state'] = le.fit_transform(df['state'])
df['area.code'] = le.fit_transform(df['area.code'])

# Display the first few rows of the dataframe
print(df.head())

"""SCALING"""

from sklearn.preprocessing import StandardScaler

# Apply scaling
scaler = StandardScaler()
df[numerical_cols] = scaler.fit_transform(df[numerical_cols])

df.head()



df.drop(columns=['intl.charge', 'eve.charge', 'night.charge','day.charge'], inplace=True)

df.head()

from sklearn.feature_selection import chi2
import numpy as np

df_encoded= df.copy()
categorical_cols=['state','area.code','voice.plan_yes','intl.plan_yes','churn_yes']
# Apply Chi-square test
chi_scores = chi2(df_encoded[categorical_cols], df_encoded['churn_yes'])
p_values = pd.Series(chi_scores[1], index=categorical_cols)

# Print features with p-values
print(p_values.sort_values())

from sklearn.ensemble import RandomForestClassifier

# Assume 'df' is your cleaned dataset, and 'churn' is the target
X = df.drop('churn_yes', axis=1)
y = df['churn_yes']

# Train a RandomForestClassifier to get feature importance
model = RandomForestClassifier()
model.fit(X, y)

# Extract feature importances
importances = model.feature_importances_

# Create a Series and sort it
importances_series = pd.Series(importances, index=X.columns)
importances_series = importances_series.sort_values(ascending=False)

# Display the most important features
print(importances_series)

df.drop(columns=['area.code','customer.calls'],inplace=True)
df.columns

# feature selection
X = df.drop('churn_yes', axis=1)
y = df['churn_yes']

"""### SVM"""

from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report,confusion_matrix

# Assuming X contains the preprocessed features and y contains the target variable (e.g., churn)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the SVM model
svm_model = SVC(kernel='linear')

# Train the model
svm_model.fit(X_train, y_train)

# Predict on the test set
y_pred = svm_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print("Confusion Matrix\n",confusion_matrix(y_test, y_pred))
print("Classification Report\n",classification_report(y_test, y_pred))

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Reds', cbar=False,
            xticklabels=['No Churn', 'Churn'], yticklabels=['No Churn', 'Churn'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""### KNN (K Nearest Neighbor)"""

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

# Split the dataset into 80% training and 20% testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
X_train.shape, X_test.shape, y_train.shape, y_test.shape

k = 5  # assuming intial value as 5
knn = KNeighborsClassifier(n_neighbors=k)

# Train the KNN model
knn.fit(X_train, y_train)

# Evaluate the classifier's performance
y_pred = knn.predict(X_test)

# Performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print(f"KNN Classifier Performance for K={k}:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred)) # Now classification_report is defined and can be used
#integrating performances

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix - KNN')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()

"""### Logistic Regression Model"""

from sklearn.linear_model import LogisticRegression

# Split the dataset into 80% training and 20% testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train.shape, X_test.shape, y_train.shape, y_test.shape

LR =  LogisticRegression()
LR.fit(X_train,y_train)

# Predict on the test set
y_pred =LR.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

# Print the results
print(f'Accuracy: {accuracy * 100:.2f}%')
print("Confusion Matrix:\n", conf_matrix)
print("\nClassification Report:\n", class_report)

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Greens', cbar=False,
            xticklabels=['No Churn', 'Churn'], yticklabels=['No Churn', 'Churn'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""Random forest classifier"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Split the dataset into 80% training and 20% testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train.shape, X_test.shape, y_train.shape, y_test.shape

# Initialize the RandomForestClassifier
rf_model = RandomForestClassifier(random_state=42)

# Train the model
rf_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

# Print the results
print(f'Accuracy: {accuracy * 100:.2f}%')
print("Confusion Matrix:\n", conf_matrix)
print("\nClassification Report:\n", class_report)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix - Random Forest')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()

import pickle
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification

# Train the Random Forest model
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Save the model
with open('churn_random_forest_model.pkl', 'wb') as file:
    pickle.dump(model, file)

pip install --upgrade streamlit

import streamlit as st
import pickle
import numpy as np

# Load the trained model
with open('churn_random_forest_model.pkl', 'rb') as file:
    model = pickle.load(file)

# Streamlit app layout
st.title("Telecommunication Churn Prediction")
st.write("Enter customer details to predict churn:")

# Save the model
with open('churn_random_forest_model.pkl', 'wb') as file:
    pickle.dump(model, file)

import streamlit as st
import joblib
import pandas as pd
import warnings
warnings.filterwarnings("ignore", message="missing ScriptRunContext")

# Load the trained Random Forest model
model = joblib.load("churn_random_forest_model.pkl")

# Define the input features
input_features = ['state', 'account.length', 'voice.messages', 'intl.mins', 'intl.calls',
                  'day.mins', 'day.calls', 'eve.mins', 'eve.calls', 'night.mins',
                  'night.calls', 'voice.plan_yes', 'intl.plan_yes']

# Streamlit app title
st.title("Customer Churn Prediction")

# User inputs for each feature
state = st.selectbox("State", list(range(52)), help="Select the state (0 to 51) representing each state including District of Columbia")
account_length = st.number_input("Account Length (days)", min_value=1, max_value=500, value=100)
voice_messages = st.number_input("Number of Voice Messages", min_value=0, max_value=100, value=10)
intl_mins = st.number_input("International Minutes", min_value=0.0, max_value=100.0, value=10.0)
intl_calls = st.number_input("Number of International Calls", min_value=0, max_value=20, value=5)
day_mins = st.number_input("Day Minutes", min_value=0.0, max_value=500.0, value=180.0)
day_calls = st.number_input("Day Calls", min_value=0, max_value=200, value=100)
eve_mins = st.number_input("Evening Minutes", min_value=0.0, max_value=500.0, value=150.0)
eve_calls = st.number_input("Evening Calls", min_value=0, max_value=200, value=100)
night_mins = st.number_input("Night Minutes", min_value=0.0, max_value=500.0, value=200.0)
night_calls = st.number_input("Night Calls", min_value=0, max_value=200, value=100)
voice_plan_yes = st.selectbox("Voice Plan (Yes=1, No=0)", [0, 1])
intl_plan_yes = st.selectbox("International Plan (Yes=1, No=0)", [0, 1])

# Organize user inputs into a dataframe for model prediction
user_data = pd.DataFrame([[state, account_length, voice_messages, intl_mins, intl_calls,
                           day_mins, day_calls, eve_mins, eve_calls, night_mins,
                           night_calls, voice_plan_yes, intl_plan_yes]], columns=input_features)

# Display prediction results
    if st.button("Predict"):
        prediction = model.predict(input_features)
        prediction_text = "will churn" if prediction[0] == 1 else "will not churn"
        st.write(f"The model predicts that this customer {prediction_text}.")